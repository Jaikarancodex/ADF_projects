# Student Analytics Pipeline â€” Guide ğŸš€ğŸ“Š

## ğŸŒŸ Project Overview
This project automates the processing of student CSV files using Azure Data Factory (ADF). When a new or updated file arrives in the `raw/` container, ADF triggers a pipeline that:

- Validates file existence
- Copies the file to `processed/` with a timestamp
- Copies a log version to `logs/`
- Records file size
- Handles missing files cleanly

Perfect for team demos, learning paths, and real-world data engineering practice!

---

## ğŸ“ Folder Structure (ADLS Gen2)

```
praccontainer/
 â”œâ”€â”€ raw/
 â”‚     â””â”€â”€ studentX.csv  
 â”œâ”€â”€ processed/
 â”‚     â””â”€â”€ studentX_20250101_102030.csv
 â””â”€â”€ logs/
       â””â”€â”€ log_studentX_20250101_102030.csv
```

---

## ğŸ› ï¸ Step 1 â€” Create Required Azure Resources

### 1ï¸âƒ£ Storage Account  
- Create ADLS Gen2-enabled storage  
- Create container `praccontainer`  
- Create folders:
  - `raw/`
  - `processed/`
  - `logs/`

### 2ï¸âƒ£ Key Vault (Optional)  
- Store secrets securely  
- Connect via Managed Identity in ADF

---

## ğŸ§© Step 2 â€” Create Linked Services in ADF

### âœ” Linked Service for ADLS Gen2  
Name: `LS_practice2sa`  
Auth: System-assigned Managed Identity  

### âœ” Optional: Linked Service for Key Vault  
Name: `LS_KeyVault`

---

## ğŸ—‚ï¸ Step 3 â€” Create Datasets

### ğŸ“Œ `ds_student_raw` (full file path)
- Parameter: `FilePath` (String)
- Connection:
  - File path: `@dataset().FilePath`

### ğŸ“Œ `ds_student_processed`
- Parameter: `TargetFile` (String)
- Connection: `processed/@dataset().TargetFile`

### ğŸ“Œ `ds_student_logs`
- Parameter: `LogFile` (String)
- Connection: `logs/@dataset().LogFile`

---

## ğŸ—ï¸ Step 4 â€” Build the Pipeline  
**Pipeline Name: `PL_StudentData_Processing`**

### Parameters:
| Name | Type | Description |
|------|--------|-------------|
| SourceFile | string | full raw path e.g. `raw/student.csv` |
| ProcessedFileNamePrefix | string | example: `student_marks` |

### Variables:
- `FileSize`
- `Status`

---

## ğŸ” Step 5 â€” Activities Overview

### 1ï¸âƒ£ Get Metadata  
Fields:
- exists  
- size  
- lastModified  
Dataset: `ds_student_raw`  
Mapping:
```
FilePath = @pipeline().parameters.SourceFile
```

### 2ï¸âƒ£ If Condition  
Expression:
```
@activity('GetMeta_CheckFile').output.exists
```

### TRUE branch â†’ Process file  
- **Copy_To_Processed**  
  - TargetFile:
    ```
    @concat(
        pipeline().parameters.ProcessedFileNamePrefix,
        '_',
        formatDateTime(utcNow(),'yyyyMMdd_HHmmss'),
        '.csv'
    )
    ```
- **Copy_To_Logs**
  - LogFile:
    ```
    @concat(
        'log_',
        last(split(pipeline().parameters.SourceFile,'/')),
        '_',
        formatDateTime(utcNow(),'yyyyMMdd_HHmmss'),
        '.csv'
    )
    ```

- **Set FileSize Variable**
  ```
  @string(activity('GetMeta_CheckFile').output.size)
  ```

### FALSE branch â†’ Missing file  
- Set variable:
  ```
  "File Missing"
  ```

---

## âš¡ Step 6 â€” Event Trigger (New or Updated Files)

### Trigger Type:
âœ” Storage events  
âœ” Container: `praccontainer`  
âœ” Path Begins With: `raw/`  
âœ” Events:
- Blob Created  
- Blob Modified  

### Trigger Parameter Mapping:
| Pipeline Param | Expression |
|----------------|------------|
| SourceFile | `@triggerBody().fileName` |
| ProcessedFileNamePrefix | `student_marks` |

---

## ğŸ§ª Step 7 â€” Testing

### âœ” Test 1: New file
Upload:  
`raw/new_students_01.csv`

Expected:
- Pipeline triggers
- Processed file appears
- Log file appears

### âœ” Test 2: Update file
Upload same file again â†’ overwrite  
Expected:
- Pipeline triggers again

---

## ğŸ‰ Final Outcome  
You now have a **fully automated**, **event-driven**, **incremental**, and **production-style** ADF pipeline.

Perfect for:
- Team learning  
- Demo presentations  
- Resume + portfolio  
- Real-world data engineering workflow  

---

## ğŸ™Œ Need More?  
I can create:
- PPT slides  
- Architecture diagrams  
- GitHub-ready README  
- Next-level upgrades (SQL, dedupe, watermarking)

Just ask! ğŸ˜ğŸ”¥
